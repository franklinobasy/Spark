# Learning Spark

Spark is a big data framework. It contains libraries for data analysis, machine learning, graph analysis and streaming live data. Spark is generally faster than Hadoop, the reason is because, hadoop writes intermediate results to disk whereas spark tries to keep intermediate results in memory whenever possible.

## Spark Context
The first component of a spark program is the "Spark Context". The spark context is the main entry point for spark functionality and it helps to connect a cluster of nodes or servers with the application.

## How to Setup Spark
Use this guide to setup your OS for pyspark - [here](https://sparkbyexamples.com/pyspark/install-pyspark-for-python/) or [here](https://medium.com/@GalarnykMichael/install-spark-on-ubuntu-pyspark-231c45677de0)

## Contents

1. [Functional Programming and Data Wrangling in Spark](https://github.com/franklinobasy/Spark/tree/master/1-Functional-Programming-Data_Wrangling-Spark)

2. [Setting Up Spark Clusters with AWS](https://github.com/franklinobasy/Spark/tree/master/2-Spark-Clusters-AWS)


3. [Debugging And Optimization](./3-Debugging-Optimization/)